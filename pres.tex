\input{./preamble.tex}

\usetheme{Madrid}

\AtBeginSection[]{
  \begin{frame}
  \vfill
  \centering
  \begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}
    \usebeamerfont{title}\insertsectionhead\par%
  \end{beamercolorbox}
  \vfill
  \end{frame}
}

\pgfplotsset{every axis/.append style={
                    axis x line=middle,    % put the x axis in the middle
                    axis y line=middle,    % put the y axis in the middle
                    axis line style={->}, % arrows on the axis
                    xlabel={$x$},          % default put x on x-axis
                    ylabel={$y$},          % default put y on y-axis
                    }}

\usefonttheme{serif}
\title{Kolmogorov-Arnold Networks}
\author{jhl}
\institute[]{@c/sim-aaa $\cdot$ @ucsd}
\date{10 June 2024}

\begin{document}

{
\setbeamertemplate{footline}{} 
\begin{frame}{}
    {\center Sim-Eval Literature Review}
    \titlepage
\end{frame}
}

\addtocounter{framenumber}{-1}

\begin{frame}{Welcome to the inaugural Sim-Eval literature review!} 
    I'm trying to keep this informal -- teaching is the best way of learning,
    and we all benefit from sharing knowledge. (I also only had a weekend to prepare.) 
    \textbf{Some bookkeeping notes:}
    \begin{itemize}
        \item Expect a bit more of the underlying intuition here, instead of the mechanical
            $\varepsilon-\delta|_{n \to \infty}$ aspects of the proofs.
        \item For simplicity, assume that the functions we care about are
            \textit{of multiple variables} and \textit{scalar-valued}, i.e. 
            \[
                f: \R^n \longrightarrow \R
            \]
        \item This talk is motivated by a recent paper by Liu, Wang et al. It is accessible
         at \url{https://arxiv.org/pdf/2404.19756v1}.
    \end{itemize}
\end{frame}

\section{A gentle (re)introduction to the multi-layer perceptron}

\begin{frame}

    The multi-layer perceptron (MLP) is the fundamental building block of
    so-called deep neural networks. Deep refers to the large ``depth'' of the network, i.e. 
    the number of layers.

    \ffig{\includegraphics[width=6cm]{deepmlp}}{\textit{A deep multi-layer perceptron network.}}
\end{frame}

\begin{frame}
    Canonically, a ``shallow'' multi-layer perceptron is \textit{two layers}. (This will be important
    for the literature!)
    A multi-layer perceptron in the shallow case is represented as
    \begin{align*}
        \hat{f} &: ~ \R^n \longrightarrow \R \\
        \hat{f}(x) &=  \sum_i^{N(\varepsilon)} a_i \sigma \left[\mathbf{W}_i \mathbf{x} + b_i \right]
    \end{align*}
    Note the width $N(\varepsilon)$ is a function of the precision $\varepsilon$.
\end{frame}

\begin{frame}
    \begin{align*}
        \hat{f}(x) &=  \sum_i^{N(\varepsilon)} a_i \sigma \left[\mathbf{W}_i \mathbf{x} + b_i \right]
    \end{align*}
    where:
    \[
        \begin{array}{c|l}
            b_i & \text{is the bias \textit{(affine!)}} \\
            \mathbf{x} & \text{is the $i$ th input vector} \\
            \mathbf{W}_i & \text{is the $i$ th weight matrix \textit{(learned!)}} \\
            \sigma & \text{is the activation function \textit{(e.g. ReLU, sigmoid, $\tanh$...)}} \\
            a_i & \text{are elements of the outermost weight matrix} \\
            N(\varepsilon) & \text{is the number of neurons}
        \end{array}
    \]
    In the shallow case, $a_i$ can be denoted by a row vector.
\end{frame}
\begin{frame}
    \begin{align*}
        \hat{f}(x) &=  \sum_i^{N(\varepsilon)} a_i \sigma \left[\mathbf{W}_i \mathbf{x} + b_i \right]
    \end{align*}
    \ffig{\includegraphics[width=6cm]{shallowmlp}}{\textit{A shallow 2-layer multi-layer perceptron.} $d=2$, $n=5$.}
\end{frame}
\begin{frame}
    More generally, for deep ($d > 2$) networks, a multi-layer perceptron is
    really just a composition of (affine!) linear transformations separated by non-linear
    activations.
    \[
        MLP(\mathbf{x}) = \left[
            \mathbf{W}_d \circ \sigma_d \circ
            \mathbf{W}_{d-1} \circ \sigma_{d-1} \circ
                \ldots
            \mathbf{W}_{1} \circ \sigma_{1} 
        \right] (\mathbf{x} )
    \]
    \ffig{\includegraphics[width=6cm]{deepmlp}}{\textit{A deep multi-layer perceptron network.}}
\end{frame}

\begin{frame}
    \[
        \hat{f}(x) =  \sum_i^{N(\varepsilon)} a_i \sigma \left[\mathbf{W}_i \mathbf{x} + b_i \right]
    \]
    \textbf{\textit{Question:}} What can a multi-layer perceptron represent? \\
    \ffig{\includegraphics[width=6cm]{shallowmlp}}{\textit{A shallow 2-layer multi-layer perceptron.} $d=2$, $n=5$.}
\end{frame}

\section{Universal Approximation}

\begin{frame}
    \textbf{\textit{Question:}} What can a multi-layer perceptron represent? \\
    \textbf{\textit{Answer}} Anything!$^*$ \\
    \vspace{1cm}
    Let $D \subset \R^n$ be compact\footnote{\tiny
        Compact denotes some notion of ``closed and bounded'' -- the $n$-dimensional
        equivalent of a closed interval $[a, b] \subset \R$.
    }.
    $f: D \longrightarrow \R$ be an \textit{arbitrary nonlinear function}, and let $\hat{f}: D \longrightarrow
    \R$ denote a shallow (\textit{n.b.} 2-layer) multi-layer perceptron, denoted by
    \[
        \hat{f}(x) =  \sum_i^{N(\varepsilon)} a_i \sigma \left[\mathbf{W}_i \mathbf{x} + b_i \right]
    \]
    where $N(\varepsilon)$ is the \textit{number of neurons}. In the shallow case this is $=$ 
    the width.
\end{frame}
\begin{frame}
    Let $D \subset \R^n$ be compact.
    $f: D \longrightarrow \R$ be an \textit{arbitrary nonlinear function}, and let $\hat{f}: D \longrightarrow
    \R$ denote a shallow (\textit{n.b.} 2-layer) multi-layer perceptron, denoted by
    \[
        \hat{f}(x) =  \sum_i^{N(\varepsilon)} a_i \sigma \left[\mathbf{W}_i \mathbf{x} + b_i \right]
    \]
    \begin{theorem}
        \textbf{Universal approximation (2-layer network).} For arbitrary $\varepsilon \in \R > 0$, there
        exists $N(\varepsilon)$ such that 
        \[
        |f(x) - \hat{f(x)}| \leq \varepsilon
        \]
    \end{theorem}
\end{frame}

\begin{frame}
    \[
        \hat{f}(x) =  \sum_i^{N(\varepsilon)} a_i \sigma \left[\mathbf{W}_i \mathbf{x} + b_i \right]
    \]
    \begin{theorem}
        \textbf{Universal approximation (2-layer network).} For arbitrary $\varepsilon \in \R > 0$, there
        exists $N(\varepsilon)$ such that 
        \[
        |f(x) - \hat{f(x)}| \leq \varepsilon
        \]
    \end{theorem}
    So we can model basically anything! But...
\end{frame}

\begin{frame}
    \begin{theorem}
        \textbf{Universal approximation (2-layer network).} For arbitrary $\varepsilon \in \R > 0$, there
        exists $N(\varepsilon)$ such that 
        \[
        |f(x) - \hat{f(x)}| \leq \varepsilon
        \]
    \end{theorem}
    ...the practical question is:
    \begin{center}
        \textbf{What is $N(\varepsilon)$?}
    \end{center}
    Is it $O(\log \varepsilon^{-1})$? $O(\varepsilon^{-1})$? $O(\exp \varepsilon^{-1})$?
\end{frame}
\section{Neural scaling}
\begin{frame}
    \textbf{What is $N(\varepsilon)$?}
    \\
    \textbf{We don't know, in general!} The universal approximation theorem guarantees no bounds on $N$. 
    For deep networks,
    we do know that it's possibly poorly behaved ($N \propto \exp(d)$, the layer depth of the network).
\end{frame}

\begin{frame}
    \textbf{Why does this make sense?} Because we are fitting a ``mostly linear'' model to an ``arbitrary
    non-linear'' function. We need ``a lot of linear pieces'' to get good at modeling funky nonlinear functions.

    \ffig{
        \begin{tikzpicture}[yscale=0.3]
            \begin{scope}
                \draw[red,thick,->] plot [domain=-3.3:3.3] ({\x}, {\x * \x});
                % \draw (3, 9) node [above right] (quad) {x^2};
            \end{scope}
            \draw[blue, <-] (3, 9) -- (2, 4) -- (1, 1) -- (0, 0) -- (-1, 1) -- (-2, 4) -- (-3, 9);
            \draw[->] (-4.8, 0) -- (4.8, 0);
            \draw[->] (0, -1.8) -- (0, 10.8);
            \draw[step=2cm,very thin,color=gray] (-4.1, -1.1) grid (4.1, 10.1);
            \draw[--] (-3, 0.3) -- (-3, -0.3) node [below=1mm] {-3};
            \draw[--] (3, 0.3) -- (3, -0.3) node [below=1mm] {3};
            \draw (3, 9) node [above,right=2mm,fill=white] (quad) {$f = x^2$};
        \end{tikzpicture}
    }{\textit{It takes a lot of line segments to approximate this quadratic, and as soon as we leave $[-3, 3]$,
    the error in our approximation blows up!}}

    Enter the notion of \textit{neural scaling laws}.
\end{frame}

\begin{frame}
    Neural scaling laws formalize the notion of ``mostly linear things approximate nonlinearities inefficiently'' --
    in particular, we can generally say that the \textit{training}\footnote{
        i.e. overfits
    } loss $\ell$ decreases according a power-law regime:
    \[
        \ell \propto N^{-\alpha}
    \]
    where $\alpha$ is the \textit{scaling exponent} and $N$ is the number of parameters. In essence,
    to decrease the loss linearly requires an exponential increase in the number of parameters.
\end{frame}

\begin{frame}
    In essence, to decrease the loss linearly requires an exponential increase in the number of parameters. \\

    Fundamentally, if the underlying process is nonlinear, then we are basically trying to fit a big piecewise
    linear function, and then combining them with a fixed nonlinearity (ReLU, sigmoid, tanh, \&c).
    \ffig{\includegraphics[width=6cm]{shallowmlp}}{\textit{The shallow perceptron again.} The nodes
        apply a fixed nonlinear activation to each input, which is a learned affine linear function
        over the incoming edges.
    }
\end{frame}

\begin{frame}
    \textbf{A practical example}: \url{go/tensorflowplayground}
    \ffig{\includegraphics[width=9cm]{tfspiral}}{\textit{Separating linearly inseparable data, with linear functions?}}
\end{frame}

\begin{frame}
    \textbf{Idea:} Why not \textit{learn the nonlinearity}? \\
    \vspace{5mm}
    Using linear piecewise bits to represent a nonlinear function seems impractical. Besides, we already
    use nonlinear basis functions to fit models (e.g. polynomial degree-$n$ regression, exponential
    regression, spline fitting). \\
    \vspace{5mm}
    This is the basic idea behind the Kolmogorov-Arnold network.
\end{frame}

\section{Kolmogorov-Arnold Networks}

\begin{frame}
    The fundamental architectural difference (other than learning the nonlinear
    `activation') between the multi-layer perceptron and the Kolmogorov-Arnold
    network is that nodes and edges are flipped:
    \ffig{\includegraphics[width=12cm]{mlpvskan}}{\textit{MLP (right) vs. KAN (left) in the shallow case.}}
    Both of these are two-layer architectures. 
\end{frame}

\section{Kolmogorov-Arnold Representation}

\begin{frame}
    The multi-layer perceptron was underpinned by the Universal Approximation theorem. The Kolmogorov-Arnold network is
    similarly underpinned by the Kolmogorov-Arnold representation theorem (surprise). 
    Let $f$ be a continuous function defined on a closed hypercube, i.e. $f : [0,1]^n \subset \R^n \longrightarrow \R$.
    \begin{theorem}
        \textbf{Kolmogorov-Arnold representation.} $f$ admits a representation that is the sum of continuous
        functions of a single variable. In particular: there exist $\phi_{q, p} : [0, 1] \longrightarrow \R$
        and $\Phi_q: \R \longrightarrow \R$ where
        \[
            f(x) = f(x_1, x_2, \ldots x_n) = \sum_{q=0}^{2n} \Phi_q \left(
                \sum_{p=1}^n \phi_{q, p} (x_p)
                \right)
        \]
    \end{theorem}
\end{frame}

\begin{frame}
    \begin{theorem}
        \textbf{Kolmogorov-Arnold representation.} $f$ admits a representation that is the sum of continuous
        functions of a single variable. In particular: there exist $\phi_{q, p} : [0, 1] \longrightarrow \R$
        and $\Phi_q: \R \longrightarrow \R$ where
        \[
            f(x) = f(x_1, x_2, \ldots x_n) = \sum_{q=0}^{2n} \Phi_q \left(
                \sum_{p=1}^n \phi_{q, p} (x_p)
                \right)
        \]
    \end{theorem}
    Well that's abstract and unhelpful! What does it mean?
\end{frame}

\begin{frame}
    Rather surprisingly, it tells us that \textit{every multivariable function defined on a compact set
    can be written with a collection of univariate functions, composed and added together}! This is a pretty 
    remarkable result:
    \begin{align*}
        & x \in [0, 1]^n \implies \\
        & \qquad \underbrace{f(x) = f(x_1, x_2, \ldots x_n)}_{\text{multivariate}}
        = \sum_{q=0}^{2n} \underbrace{\Phi_q}_{\text{univariate}} \left(
            \sum_{p=1}^n \underbrace{\phi_{q, p}}_{\text{univariate}} (x_p)
            \right)
    \end{align*}
\end{frame}

\begin{frame}
    \begin{align*}
        & x \in [0, 1]^n \implies \\
        & \qquad \underbrace{f(x) = f(x_1, x_2, \ldots x_n)}_{\text{multivariate}}
        = \sum_{q=0}^{2n} \underbrace{\Phi_q}_{\text{univariate}} \left(
            \sum_{p=1}^n \underbrace{\phi_{q, p}}_{\text{univariate}} (x_p)
            \right)
    \end{align*}
    There is a catch: we have no guarantees on how nicely-behaved these univariate functions are.
    It could well be the case that our $\phi, \Phi$ are nondifferentiable or even fractal.
\end{frame}

\begin{frame}
    Fortunately, in practice, (it turns out) if you're modeling a real-life process, the odds are good that they will be 
    (reasonably) well-behaved. \\
    \vspace{5mm}
    In addition, (in much the same way as a deep MLP network,) we can stack layers of these KAN networks together.
    \ffig{\includegraphics[width=6cm]{deepkan}}{\textit{A deep KAN.}}
\end{frame}

\begin{frame}
    \ffig{\includegraphics[width=6cm]{deepkan}}{\textit{A deep KAN.}}
    Empirically, it turns out these have high approximating power (among other benefits!).
    A deep KAN admits a very simple representation:
    \[
        \text{KAN}(x) = (\phi_n \circ \phi_{n-1} \circ \ldots \circ \phi_1)(x)
    \]
    Compare to the deep MLP formulation:
    \[
        MLP(\mathbf{x}) = \left[
            \mathbf{W}_d \circ \sigma_d \circ
            \mathbf{W}_{d-1} \circ \sigma_{d-1} \circ
                \ldots
            \mathbf{W}_{1} \circ \sigma_{1} 
        \right] (\mathbf{x} )
    \]
\end{frame}

\begin{frame}
    From the previous discussion about neural scaling, we might expect a KAN to
    be \textit{more efficient} at representing nonlinear processes than a deep MLP.
    \\ \vspace{5mm}
    \textbf{Is this true?}
\end{frame}

\begin{frame}
\end{frame}

\end{document}
